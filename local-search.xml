<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>训练神经网络（二）</title>
    <link href="/posts/ae122e4c9042/"/>
    <url>/posts/ae122e4c9042/</url>
    
    <content type="html"><![CDATA[<h1>数据预处理</h1><p>对于一个数据矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span> ，假设数据的大小为  <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>N</mi><mo>∗</mo><mi>D</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[N * D]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mclose">]</span></span></span></span>， <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> 表示数据的个数， <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span></span></span></span> 表示数据的维度。</p><h2 id="zero-centered零中心化">zero-centered零中心化</h2><p><strong>对于数据的每个特征属性，减去这个属性的平均值。其集合解释是，沿着每个维度将数据云放在以原点为中心的周围，也就是将数据进行零中心化（zero-centered）。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">X -= np.mean(X, axis = <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><h2 id="Normalization-归一化">Normalization 归一化</h2><p>归一化是指对数据维度进行归一化，使其具有大致相同的尺度。有两种常见的方法来实现这种归一化。一种是将每个维度除以其标准差：<code>X /= np.std(X, axis = 0)</code>。这种预处理的另一种形式是对每个维度进行归一化处理，使其在该维度下的取值为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[-1, 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">−</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span> 。只有当你确信，不同的输入数据具有不同的尺度（或单位），但应该在模型学习时对这些尺度上的差异一视同仁时，应用这种预处理才有意义。在输入数据为图像时，像素的相对尺度已经大致相等（在0到255的范围内），所以严格来说，没有必要执行这个额外的预处理步骤。</p><p>如下图左是原始数据，中间是 Mean subtraction后的结果，数据呈现零中心化，右边是进行归一化的结果，在每个维度上，数据的尺度相同。</p><p><img src="%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%BA%8C%EF%BC%89/image-20250701190024160.png" alt="数据预处理"></p><h2 id="批量归一化">批量归一化</h2><p>在神经网络训练中，常用批量归一化（BatchNorm）来加速收敛、稳定训练过程。</p><h2 id="PCA和白化">PCA和白化</h2><p>具体见《PCA和白化》</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>神经网络</tag>
      
      <tag>预处理</tag>
      
      <tag>正则化</tag>
      
      <tag>归一化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>训练神经网络（一）</title>
    <link href="/posts/355d99b7802b/"/>
    <url>/posts/355d99b7802b/</url>
    
    <content type="html"><![CDATA[<h2 id="神经网络结构">神经网络结构</h2><h3 id="1-层状结构">1 层状结构</h3><p><strong>神经网络是以神经元组成的图：</strong></p><p>神经元以无环图相连形成一个神经网络。换句话说，一些神经元的输出会作为一些神经元的输入。环状结构是不允许出现在神经网络中的，因为这回使得前向传播时出现无限的循环。神经网络中的神经元一般以几个不同的层的形式组织起来。对于一般的神经网络，最常见的层就是全连接层，两个相邻的全连接层中的每个神经元都和其他层中的所有神经元相连，但是同一层中的神经元不会相互连接。如下图是两种全连接的神经网络的示意图：</p><p><img src="https://img.fangkaipeng.com/blog_img/20220714141215.png" alt="image-20220714141208510"></p><p><strong>命名规范：</strong></p><p>一般来说我们说的n层神经网络是不包括输入层的，因此，单层神经网络用来描述没有隐藏层只有输出层的神经网络。在这种意义上来说，你可以听到有时候人们会说logistic回归和SVM是一种特殊的单层神经网络。你可能还会听到这类网络被描述成：Artificial Neural Networks ANN 人造神经网络，Multi-Layer Perceptrons MLP多层感知机。许多人不喜欢神经网络和真正的大脑之间的类比，而是更喜欢将神经元称为单位。</p><p><strong>输出层：</strong></p><p>不同于神经网络中的其他层，输出层的神经元一般不会包含激活函数，因为最后一层的输出一般用于表示不同类别的得分，一般是任意的实数。</p><p><strong>计算神经网络的大小：</strong></p><p>人们一般使用两个指标来计算神经网络的大小，即神经元的个数，或者更常用的是参数个数，下面计算上图网络中的这两个指标值：</p><ul><li>左图，有 4+2=6 个神经元（不计算输入层的神经元），有 3∗4+4∗2=20 个权重， 4+2=6 个偏置，一共 26 个参数。</li><li>右图，有 4+4+1=9 个神经元（不计算输入层的神经元），有  3∗4+4∗4 + 4∗1=32 个权重， 4+4+1=9 个偏置，一共 41 个参数。</li></ul><p>作为比较，卷积神经网络一般有1亿个参数，一般由10-20层组成，并且由于权值共享，实际有效的连接会更多。</p><h3 id="2-前向传播示例">2 前向传播示例</h3><p><strong>矩阵乘积与激活函数重复地交织在一起：</strong></p><p>神经网络被组织成层状结构的其中一个主要原因是这一结构使得使用矩阵乘法来评估神经网络变得更为简单和高效。如上图中的右图3层神经网络，输入是一个 的向量。全部的神经元之间连接的权值可以被存放在一个矩阵中。比如说，第一个隐藏层的权重 将是一个 的矩阵，并且所有偏置项可以组成一个 的向量 。这里，每个神经元的全部权重表示成了 的一列，所以这一层的输出可以用点乘 <code>np.dot(W1,x)</code> 来表示。如下是使用矩阵乘法进行前向传播的过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># forward-pass of a 3-layer neural network:</span><br>f = <span class="hljs-keyword">lambda</span> x: <span class="hljs-number">1.0</span>/(<span class="hljs-number">1.0</span> + np.exp(-x)) <span class="hljs-comment"># activation function (use sigmoid)</span><br>x = np.random.randn(<span class="hljs-number">3</span>, <span class="hljs-number">1</span>) <span class="hljs-comment"># random input vector of three numbers (3x1)</span><br>h1 = f(np.dot(W1, x) + b1) <span class="hljs-comment"># calculate first hidden layer activations (4x1)</span><br>h2 = f(np.dot(W2, h1) + b2) <span class="hljs-comment"># calculate second hidden layer activations (4x1)</span><br>out = np.dot(W3, h2) + b3 <span class="hljs-comment"># output neuron (1x1)</span><br></code></pre></td></tr></table></figure><h3 id="3-表达能力">3 表达能力</h3><p>有一种说法是，使用全连接层的神经网络可以看成通过权值来参数化得定义了一系列的函数。那么这一系列的函数的表达能力如何呢？换句话说，是否存在一个函数时不能由神经网络表示的？</p><p>其结论是，包含至少一个隐藏层的神经网络是一个通用的近似表达器（universal approximator）。也就是说，对于任意一个连续的函数 ，以及 ，存在一个拥有一个隐藏层的神经网络 ，可以保证 ，换句话说，神经网络可以近似表达任何连续函数。</p><p>如果一个隐藏层足以近似任何函数，为什么要使用更多的层使得网络更深呢？答案是，两层神经网络是通用的近似值，是一个在数学上很可爱但在实际应用中较弱且无用的事实。比如 sum of indicator bumps 函数 ，也是一种通用近似表达器，但是没人会将其用于机器学习中。神经网络在实践中效果很好，因为它们可以简洁地很好地表示一个平滑函数，并且可以很好地表示出我们在实践中遇到的数据的统计特性，同时我们可以很简单地使用优化算法（比如梯度下降）。同样的，尽管单个隐藏层的神经网络和更深的神经网络的表达能力是相同的，但是实际上更深的网络的效果会更好。</p><p>此外，通常在实际应用中3层的网络效果比2层的更好，但是更深的网络所带来的增益就很小了。这和卷积神经网络不同，在卷积神经网络中，深度对于一个好的识别系统是非常重要的。对于此现象的一种解释是，图片拥有分层的结构（比如说脸是由眼睛组成的，而眼睛又是由一些边缘组成的），所以层数对于这样的数据域具有直观的意义。</p><h3 id="4-设置层数和每层的大小">4 设置层数和每层的大小</h3><p>面对一个实际问题，我们应该如何去构造一个网络呢？我们需要使用隐藏层吗？需要使用几个隐藏层？每层应该设置成多大？首先，我们需要知道的是，当我们增加模型的层数和每层的大小时，模型的能力都会有所增加。也就是说，可以表达的函数空间增加了，因为神经元相互合作可以表达很多不同的函数。比如说，假设我们在二维空间中有一个二元分类问题，我们可以训练三个不同的网络，每个神经网络都只有一个隐藏层，可视化结果如下，此结果可以在 <a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html">ConvNetsJS demo</a> 中自己训练得到：</p><p><img src="https://img.fangkaipeng.com/blog_img/20220714151526.png" alt="image-20220714151526307"></p><p>在上图中，我们可以看到，同样层数的神经网络，使用更多的神经元可以表示更复杂的函数。不过，但是，这既是祝福（因为我们可以学会对更复杂的数据进行分类），又是诅咒（因为它很容易在训练集中发生过拟合）。过拟合现象一般发生在模型具有很强的能力，于是可以拟合数据中的噪音，而不是拟合数据间潜在的关系。比如，具有20个隐藏神经元的模型符合所有训练数据，但代价是将空间细分为了许多不相交的红色和绿色决策区域。具有3个隐藏神经元的模型仅具有对数据的宽泛的表达能力。它认为数据由两部分组成，少量在绿色区域中的红点被认为是一些噪音。在实际中，这样的分类器在测试集中具有较好的泛化能力。</p><p>基于上面的讨论，如果数据不是很复杂，为了防止过度拟合，则似乎可以优选较小的神经网络。不过，这是不正确的，这里有很多其他的方法用于避免过拟合。在实践中，使用这些方法来防止过拟合比减少神经元个数的方法更好。</p><p>背后的微妙原因是，较小的网络很难使用局部方法（例如梯度下降）进行训练：显而易见的是，他们的损失函数之间只有相对少的局部最小值，并且这些局部最小值很容易造成收敛，这不是一件好事（因为会伴随很大的loss）。相反的，使用更大的神经网络将会包含更多重要的局部最小值，这些局部最小值会比它们实际的损失值更好。实际上，如果你训练一个小型的网络，那么最终的损失值会呈现出一个很大的差异，因为有时候你比较幸运收敛到了一个较好的局部最小值，而有时候你运气比较差，收敛到了一个不太好的局部最小值。另外一方面，如果训练大型的网络，你会发现你可能会有很多种解决方案，但是最终损失值的差异都会很小。换句话说，所有的解决方法效果都是一样好的，并且更少地依靠于一开始的随机初始化的运气。</p><p>再次声明，控制正则化强度是一种更好的控制过拟合的方法，而不是控制神经元的个数。从下图可以观察，不同正则化强度的差异：</p><p><img src="https://img.fangkaipeng.com/blog_img/20220714153550.png" alt="image-20220714153550518"></p><p>关键点在于，你可能害怕发生过拟合而使用较小的网络，这是不正确的 。取而代之的是，你应该按照设备的计算能力来设计足够大的神经网络，并使用其他正则化技术来控制过度拟合。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>神经网络</tag>
      
      <tag>CS231n</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>激活函数</title>
    <link href="/posts/66179ecfa848/"/>
    <url>/posts/66179ecfa848/</url>
    
    <content type="html"><![CDATA[<h2 id="什么是激活函数">什么是激活函数</h2><p><strong>激活函数（Activation Function）是一种添加到人工神经网络中的函数，旨在帮助网络学习数据中的复杂模式。在神经元中，输入的input经过一系列加权求和后作用于另一个函数，这个函数就是这里的激活函数</strong>。</p><p>激活函数可以分为<strong>线性激活函数</strong>（线性方程控制输入到输出的映射，如f(x)=x等）以及<strong>非线性激活函数</strong>（非线性方程控制输入到输出的映射，比如Sigmoid、Tanh、ReLU、LReLU、PReLU、Swish 等）</p><h2 id="为什么要用激活函数">为什么要用激活函数</h2><p>神经网络中每一层的输入输出都是一个线性求和的过程，下一层的输出只是承接了上一层输入函数的线性变换，所以如果没有激活函数，那么无论你构造的神经网络多么复杂，有多少层，最后的输出都是输入的线性组合，纯粹的线性组合并不能够解决更为复杂的问题。而引入激活函数之后，我们会发现常见的激活函数都是非线性的，因此也会给神经元引入非线性元素，使得神经网络可以逼近其他的任何非线性函数，这样可以使得神经网络应用到更多非线性模型中。</p><h2 id="常见的激活函数">常见的激活函数</h2><h3 id="Sigmoid函数">Sigmoid函数</h3><p><img src="https://img.fangkaipeng.com/blog_img/20220713135205.png" alt="img"></p><p>Sigmoid函数的数学表达形式为：</p><p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>σ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\sigma(x) = \frac{1}{1 + e^{-x}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.0908em;vertical-align:-0.7693em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6973em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><p>它接受一个实数并将其压缩到0-1的范围内，其中大的负数将会变成0，而大的正数将会变成1。sigmoid函数有两个主要的缺点：</p><ul><li><strong>导致梯度饱和或消失：</strong> 当输入值很大或很小时，sigmoid函数的导数接近0。而在反向传播的时候，这个局部梯度将会和上游传来的梯度进行相乘。因此，如果局部梯度非常小，那么它将“杀死”梯度，使得几乎没有信号从这个神经元中传出，导致训练缓慢或停滞。此外，初始化的时候也需要保持谨慎，以防止一开始的时候神经元就出现梯度消失的问题。</li><li><strong>输出不是零均值化的（zero-centered）：</strong> sigmoid输出范围是(0,1)，不是以0为中心。如果输入的数据一直保持正的，那么在反向传播时，权值 w 的梯度将会全部为正或全部为负（取决于上游传来的梯度正负）。这就导致根据梯度更新参数时产生Z字抖动，即只能朝着两个特定的方向移动，不能朝着正确的方向移动。</li></ul><h3 id="tanh函数">tanh函数</h3><p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>tanh</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>−</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">tanh</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.2177em;vertical-align:-0.7693em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.4483em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.5904em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6973em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7713em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><p><img src="https://img.fangkaipeng.com/blog_img/20220713141018.png" alt="image-20220713141018720"></p><p>​    梯度消失问题仍然存在</p><h3 id="ReLU函数">ReLU函数</h3><p>下图左是ReLU函数图像，右图是来自论文 <a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf">Krizhevsky et al.</a> 中将ReLU的训练效果与Tanh的比较，有6倍的提升。</p><p><img src="https://img.fangkaipeng.com/blog_img/20220713141320.png" alt="image-20220713141320164"></p><p>ReLU全称为Rectified Linear Unit，整流线性单元，这在最近几年非常流行。它的公式为 ，换句话说，激活与否的门槛被简单得设置成了0，ReLu函数有以下几个优劣：</p><ul><li><strong>大大加速收敛速度：</strong> 与Tanh和Sigmoid相比，ReLU在随机梯度下降时的收敛速度更快，有人认为这是由于其一半线性一半不饱和形式所致。</li><li><strong>更高效：</strong> 与Sigmoid和Tanh相比，ReLU函数的计算速度更快。</li><li><strong>较为脆弱：</strong> 存在Dead Relu Problem，即某些神经元可能永远不会被激活，进而导致相应参数一直得不到更新，产生该问题主要原因包括参数初始化问题以及学习率设置过大问题</li></ul><h3 id="Leaky-ReLu">Leaky ReLu</h3><p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>x</mi><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>x</mi><mo>≥</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>α</mi><mi>x</mi><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>x</mi><mo>&lt;</mo><mn>0</mn></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">f(x) = \begin{cases}x, &amp; x \geq 0 \\\\\alpha x, &amp; x &lt; 0\end{cases}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:4.32em;vertical-align:-1.91em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.35em;"><span style="top:-2.2em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎩</span></span></span><span style="top:-2.192em;"><span class="pstrut" style="height:3.15em;"></span><span style="height:0.316em;width:0.8889em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.8889em" height="0.316em" style="width:0.8889em" viewBox="0 0 888.89 316" preserveAspectRatio="xMinYMin"><path d="M384 0 H504 V316 H384z M384 0 H504 V316 H384z"/></svg></span></span><span style="top:-3.15em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎨</span></span></span><span style="top:-4.292em;"><span class="pstrut" style="height:3.15em;"></span><span style="height:0.316em;width:0.8889em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.8889em" height="0.316em" style="width:0.8889em" viewBox="0 0 888.89 316" preserveAspectRatio="xMinYMin"><path d="M384 0 H504 V316 H384z M384 0 H504 V316 H384z"/></svg></span></span><span style="top:-4.6em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎧</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.85em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.41em;"><span style="top:-4.41em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mpunct">,</span></span></span><span style="top:-2.97em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"></span></span><span style="top:-1.53em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal">αx</span><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.91em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.41em;"><span style="top:-4.41em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">0</span></span></span><span style="top:-1.53em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.91em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><p>Leaky ReLU函数是对解决ReLU神经元死亡的一种尝试。与ReLU对负数全变为0不同，leaky ReLU给负半边一个很小的正斜率，也就是说，表达式变成 ，其中的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">a</span></span></span></span>是一个很小的常数。有些人报告了这种激活函数的成功，但结果并不总是一致的。此外，负半边区域中的斜率 也可以成为每个神经元的一个参数进行学习得来，这就是PReLU</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/posts/cf42a04f2e55/"/>
    <url>/posts/cf42a04f2e55/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start">Quick Start</h2><h3 id="Create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
